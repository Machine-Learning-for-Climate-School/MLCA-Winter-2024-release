{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f6025b5d-0d19-4331-b242-adec33e939e4",
   "metadata": {
    "tags": []
   },
   "source": [
    "# **Exercise session 3: Convolutional neural network (CNN) and support vector machine (SVM) to detect CH$_4$ emissions from satellite data**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5cefeb3a-9dde-482e-b9ce-46942eda92e8",
   "metadata": {},
   "source": [
    "**Reference:** Schuit, Berend J., et al. \"Automated detection and monitoring of methane super-emitters using satellite data.\" Atmospheric Chemistry and Physics Discussions (2023): 1-47.e\n",
    "\n",
    "**Data:** \n",
    "- Sentinel 5P, bands CH4, CO, SO2 and NO2\n",
    "- Sentinel 3, bands S5 and S6\n",
    "- [all TROPOMI detected plumes for 2021. (Schuit et al. 2023)](https://zenodo.org/records/8087134)\n",
    "\n",
    "**Useful tutorials:**\n",
    "- [SVM for Multiclass Classification](https://www.kaggle.com/code/pranathichunduru/svm-for-multiclass-classification)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0495d954",
   "metadata": {},
   "source": [
    "This exercise is designed to be completed on Aalto JupyterHub. Please ensure that your notebook includes all necessary installation commands for any additional libraries your code requires. These commands should be clearly written and integrated within your notebook. To submit, go to Nbgrader/Assignment List and click submit next to the exercise. \n",
    "All data loaded from Copernicus Dataspace, should be saved to /coursedata/users/$USER folder.\n",
    "\n",
    "The exercise consists of two parts:\n",
    "- In Part I (Ex. 3.1 - 3.2) you will discuss and compare the approaches to designing automated emissions monitoring algorithms in Finch et al. (2022), Schuit et al. (2023) and your own in Assignment 2\n",
    "- In Part II (Ex. 3.3 - 3.8) you will use the TROPOMI detected plumes for 2021 (Schuit et al. 2023) to construct a multiclass SVM algorithm to distinguish between oil, gas and coal emissions.\n",
    "\n",
    "The deadline is March 1 at 10:00. We will be grading the submissions as they arrive, so if you submit before the deadline, you will most likely get feedback earlier."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5eb66d3c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import necessary modules. If any additional modules need to be installed to run it on Aalto JupyterHub, include all necessary installation commands."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34d5ad94",
   "metadata": {},
   "source": [
    "## Exercise 3.1 Convolutional Neural Networks for detecting gas plumes (2 pt)\n",
    "\n",
    "Compare the approaches takes in Finch et al. (2022), Schuit et al. (2023) and your own in Assignment 2. What are the strength and weaknesses of each approach? Consider both data preparation and model design."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c3299b5",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "db131903",
   "metadata": {},
   "source": [
    "## Exercise 3.2 Trustworthy monitoring of emissions (2 pt)\n",
    "\n",
    "One design requirement for introducing an automated monitoring system of emissions is that this system is trustworthy. The EC High-Level Expert Group on AI have developed the [Ethics Guidelines for Trustworthy Artificial Intelligence](https://digital-strategy.ec.europa.eu/en/library/ethics-guidelines-trustworthy-ai) that outline 7 key requirements that AI system needs to satisfy in order to be deemed trustworthy. Discuss how well the approaches in Finch et al. (2022), Schuit et al. (2023) and Assignment 2 satisfy those requirements. You can discuss all three approaches more generally or have a deeper focused discussion about one of them.\n",
    "\n",
    "(Optional) If you interested to learn more, here are some references:\n",
    "- [Deliverables of the High-Level Expert Group on AI](https://digital-strategy.ec.europa.eu/en/policies/expert-group-ai)\n",
    "- Review papers on bias in ML: \n",
    "    - Mehrabi, Ninareh, et al. \"A survey on bias and fairness in machine learning.\" ACM computing surveys (CSUR) 54.6 (2021): 1-35.\n",
    "    - Chakraborty, Joymallya, Suvodeep Majumder, and Tim Menzies. \"Bias in machine learning software: Why? how? what to do?.\" Proceedings of the 29th ACM Joint Meeting on European Software Engineering Conference and Symposium on the Foundations of Software Engineering. 2021."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6042bb57",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "7d9e84dc",
   "metadata": {},
   "source": [
    "## Exercise 3.3 Batch jobs and server-side computations using OpenEO (1 pt)\n",
    "\n",
    "So far you have been using openEO for synchronous execution of your requests: you submitted a request and the result came as a direct response to your request. This is not feasible for heavier work. Instead, one should submit the requests as batch jobs. Familiarise yourself with the Batch-job-combine-tasks.ipynb file.\n",
    "\n",
    "Furthermore, instead of downloading large amounts of raw data, one can process the data directly on server side. You can check all available processes by running 'connection.list_processes()'. For more details, see the [EO Cookbook](https://openeo.org/documentation/1.0/cookbook/#temporal-mean-reduce-dimension). Note that they give two alternatives on how to compute the temporal mean and we recommend to use the 'reduce_dimension' function.\n",
    "\n",
    "Task: fix an area of interest and a time period and load the min, max, mean and standard deviation of CH4. Perform this task as a batch job using server-side processes. You should save the output in the JSON format. Note that the output is computed per pixel."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "afb6d0ef",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "186fcc46",
   "metadata": {},
   "source": [
    "## Exercise 3.4 Data preparation (2 pt)\n",
    "\n",
    "Load the coordinates and labels of the CH4 plumes coming from gas infrastructure, oil infrastruture and coal mines from [all TROPOMI detected plumes for 2021. (Schuit et al. 2023)](https://zenodo.org/records/8087134).\n",
    "\n",
    "**In total, there are 1913 plumes from gas infrastructure, oil infrastruture and coal mines. You can choose >=50 plumes (approximately equally distributed between the three categories). Feel free to divide and conquer (see the file Batch-job-combine-tasks.ipynb) for suggestions. If you split the task with other students, each should download the data for at least 20 plumes.**\n",
    "\n",
    "For each plume, compute a 0.5 deg x 0.5 deg rectangle around the plume. We will assume that if a point source is a super-emitter, then it's emissions are noticeable almost every day. We are going to generate features based on the observation of CH4, CO, SO2 and NO2 from Sentinel 5P data and S5 and S6 from Sentinel 3 SLSTR data observed during Jan 2021 over each rectangle. Reasons for choosing these features:  \n",
    "- CO, SO2 and NO2 are co-emitted in oil&gas industry and coal mines. See, for example, [Fioletov et al. (2016)](https://doi.org/10.5194/acp-16-11497-2016) and [Trenchev et al. (2023)](https://doi.org/10.3390/rs15061590)\n",
    "- Methane has a \"spectral fingerprint\" - a unique way of absorbing infrared light, which can be used to identify emitters using satellites, which were not originally intended for tracking methane. An additional benefit of this approach is that such sattelites usually have higher resolution, so it is easier to pinpoint the point sources. See: [publication about NASA's EMIT mission](https://www.nasa.gov/centers-and-facilities/jpl/methane-super-emitters-mapped-by-nasas-new-earth-space-mission/) and [Pandey et al. (2023)](https://doi.org/10.1016/j.rse.2023.113716).\n",
    "\n",
    "You need to:\n",
    "1. Using batch jobs and server-side processes (aggregate_spatial, see details in Batch-job-combine-tasks.ipynb), compute a time-series with mean observations over each rectangle for Jan 2021.\n",
    "2. Locally, compute [min, max, mean, sd and median] over Jan 2021 for the mean-over-rectangle for CH4, CO, SO2 and NO2 from Sentinel 5P and S5 and S6 from Sentinel 3 SLSTR data.\n",
    "\n",
    "In the end, each plume should have 5x6 features: [min_CH4_Jan_2021,...,median_S6_Jan_2021].\n",
    " \n",
    "Notes:\n",
    "- Sentinel 3 has significantly higher resolution than Sentinel 5P. Hence, your image for Sentinel 3 will contain significantly more pixels than the one for Sentinel 5P for the same spatial and temporal extents\n",
    "- If you use job=datacube.create_job() and then job.start() then the batch jobs are performed asynchronously, meaning you can shut down the device and retrieve the results later (within 7 days). To give you a sense of approx. time: Sentinel 5P computations need to be done individually per band but for Sentinel 3 the bands can be combined. You can run at most 2 simultaneous batch jobs on Copernicus servers. Hence, in total, you can complete all batch jobs in 3 attempts. When I computed the example in Batch-job-combine-tasks.ipynb for two rectangles for CH4, the job took about 2 mins to complete but the actual running time was 74 seconds (rest is queueing)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e97f2ce",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "7c7c2e82",
   "metadata": {},
   "source": [
    "## Exercise 3.5 SVM (4 pt)\n",
    "\n",
    "Build an SVM model using the data you prepared in Exercise 3.4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53751feb",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "5f9f654f",
   "metadata": {},
   "source": [
    "## Exercise 3.6 Discuss the approach you took in Exercises 3.4 and 3.5. (2 pt)\n",
    "\n",
    "Some points to consider:\n",
    "- Strength and weaknesses? \n",
    "- Suggestions for improvement? \n",
    "- Implications for policy-makers? \n",
    "- Possible further research questions?\n",
    "- Ethical considerations?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f783a6c4",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "9d873875",
   "metadata": {},
   "source": [
    "## (Optional) Exercise 3.7 Use any other (supervised or unsupervised) classification algorithm on data you prepared in Exercise 3.4 (3 pt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d20341d1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "afa98fcd",
   "metadata": {},
   "source": [
    "Why did you choose this algorithm? Compare to the approach and results in Exercise 3.5."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ff46f1d",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "8082ef9d",
   "metadata": {},
   "source": [
    "## (Optional) Exercise 3.8 Test the model from Exercise 3.5 against a different time period (3 pt)\n",
    "\n",
    "Assume that the location of super-emitters remains relatively stable over time. As in Exercise 3.4 load the data for the point sources but now for a different time period (say, Jan of another year or another month in 2021). Test how well your model from Exercise 3.5 performs on this new data. Discuss"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "faa3f18f",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
