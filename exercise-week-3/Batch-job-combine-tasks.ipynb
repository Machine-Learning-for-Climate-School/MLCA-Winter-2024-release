{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3fc595db-1a39-49fd-82fa-9f88faaebccb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import openeo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a939ae0-090a-4174-a67a-6609bc6c24b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "connection = openeo.connect(url=\"openeo.dataspace.copernicus.eu\")\n",
    "connection.authenticate_oidc()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc868765",
   "metadata": {},
   "source": [
    "## Batch jobs basics\n",
    "\n",
    "Let's do a simple batch job first: compute pixel-wise means over an area."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "687c716f-e283-476e-90c8-450ae12a393b",
   "metadata": {},
   "source": [
    "1. Specify the range of raw data that need to be loaded by the server for further processing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c92e1c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "period=(\"2022-05-01\", \"2022-06-01\")\n",
    "aoi={\n",
    "        \"west\": 3.20,\n",
    "        \"south\": 51.18,\n",
    "        \"east\": 3.70,\n",
    "        \"north\": 51.68,\n",
    "        \"crs\": \"EPSG:4326\",\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2e9691e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Server-side computations with Sentinel 5 data process one band at a time\n",
    "s5_CH4 = connection.load_collection(\n",
    "    \"SENTINEL_5P_L2\",\n",
    "    spatial_extent=aoi,\n",
    "    temporal_extent = period,\n",
    "    bands=[\"CH4\"]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cca8781d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Server-side computations with Sentinel 3 data process several bands at a time\n",
    "s3_all_bands = connection.load_collection(\n",
    "    \"SENTINEL3_SLSTR\",\n",
    "    spatial_extent=aoi,\n",
    "    temporal_extent = period,\n",
    "    bands=[\"S5\", \"S6\"]\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f115d288",
   "metadata": {},
   "source": [
    "2. Define the server-side computations that need to be performed on the raw data. For now, we are just computing pixel-wise means. We will make this more advanced in a moment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f2496e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "s5_CH4_mean = s5_CH4.reduce_dimension(dimension=\"t\", reducer=\"mean\")\n",
    "s5_CH4_mean = s5_CH4_mean.save_result(format=\"JSON\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee41ab32",
   "metadata": {},
   "outputs": [],
   "source": [
    "s3_mean = s3_all_bands.reduce_dimension(dimension=\"t\", reducer=\"mean\")\n",
    "s3_mean = s3_mean.save_result(format=\"JSON\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b2d2c23e",
   "metadata": {},
   "source": [
    "3. Create a batch job. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1596fdf5",
   "metadata": {},
   "outputs": [],
   "source": [
    "job=s3_mean.create_job(title=\"Mean-S3-aoi\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "092178b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "job = s5_CH4_mean.create_job(title=\"Mean-S5-CH4-aoi\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc324e63",
   "metadata": {},
   "source": [
    "Batch jobs allow you to start a job, leave (and shut down your device), let it run on the servers and then reconnect later. For that, you need to know the job_id."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a350648f",
   "metadata": {},
   "outputs": [],
   "source": [
    "job_id=job.job_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64d28d6a",
   "metadata": {},
   "outputs": [],
   "source": [
    "Start the job. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "219d08fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "job.start()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "49f00b95",
   "metadata": {},
   "source": [
    "Note that execute_batch() can both create and start the job for you but it is not suitable for long-running jobs, because you will need to keep your client running all the time.\n",
    "To reconnect to a job at a later time, do"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1cba1b71",
   "metadata": {},
   "outputs": [],
   "source": [
    "job = connection.job(job_id)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "51db6580",
   "metadata": {},
   "source": [
    "Useful commands for exploring your jobs:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f87a16a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# list your batch jobs\n",
    "connection.list_jobs()\n",
    "# check the status of the job\n",
    "job.status()\n",
    "# check the logs\n",
    "job.logs()\n",
    "# cancel a job\n",
    "job.stop()\n",
    "# see job details, including the cost ( see monthly quotas https://documentation.dataspace.copernicus.eu/Quotas.html) and a visual representation of how the job was constructed\n",
    "job \n",
    "# job meta data\n",
    "job.describe()\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3150bf48",
   "metadata": {},
   "source": [
    "Download the results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6eab69a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "PATH=\n",
    "results = job.get_results()\n",
    "results #to get the properties of the results\n",
    "#results.download_files(PATH) #to download all files (including json with job metadata)\n",
    "results.download_file(\"path with file name or file name\") #to download a specific file"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d5ae593",
   "metadata": {},
   "source": [
    "## Combine locations and compute mean per area of interest for a given timestamp\n",
    "\n",
    "We are going to use the function aggregate_spatial(). It takes a list of areas of interest and compute the statistics per area (not pixel-wise as before!) for each time stamp. The function takes three arguments:\n",
    "1. data cube (the raw data the needs to be pre-loaded by servers to run the computations). One can drop spatial_extent from the definition of the data cube. However, if you know that all of your areas of interest (=rectangles surrounding the plumes) are located in some bounding box (e.g., you only want to study the plumes over the EU), then specifying that bounding box in spatial extent will speed up the computations.\n",
    "2. set of areas of interest\n",
    "3. reducer - a function, that is going to be applied to each area of interest at a given timestamp. In our case, a timestamp corresponds to a day and we are going to be computing the means. I.e, for rectangle A on Day B we compute mean_{all pixels in A}(value at pixel)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4af360bc",
   "metadata": {},
   "source": [
    "**1. Define the datacube**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1d0cc92",
   "metadata": {},
   "outputs": [],
   "source": [
    "s5_CH4 = connection.load_collection(\n",
    "    \"SENTINEL_5P_L2\",\n",
    "    #spatial_extent=optional_bounding_box_for_all_plume_rectangles,\n",
    "    temporal_extent = period,\n",
    "    bands=[\"CH4\"] #as before, server-side computations for S5 accept only one band at a time\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "876c701a",
   "metadata": {},
   "outputs": [],
   "source": [
    "s3_all_bands = connection.load_collection(\n",
    "    \"SENTINEL3_SLSTR\",\n",
    "    #spatial_extent=optional_bounding_box_for_all_plume_rectangles,\n",
    "    temporal_extent = period,\n",
    "    bands=[\"S5\", \"S6\"] #as before, server-side computations for S3 accept only several bands at a time\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8facd914",
   "metadata": {},
   "source": [
    "**2. Pass the set of areas of interest**\n",
    "\n",
    "We are going to be using the aggregate_spatial function in OpenEO to compute the statistics for several areas at one.\n",
    "\n",
    "The information about the areas of interest (in our case, 0.5 x 0.5 deg polygons centered at the plume coordinates), need to be passed using the GeoJSON format. You can use the template below and just change the coordinates. Note:\n",
    "- Usually, one gives coordinates in the format (lat, lon) but in GeoJSON the order is reversed, so you need to write (lon, lat)\n",
    "- A rectangle in GeoJSON has 5 vertices: the first and the last should be the same (you need to close the polygon). It is important that the format of the first and last vertex is exactly the same.\n",
    "- The vertices need to be entered in counter-clockwise order starting from the South-Eastern vertex and have the form \n",
    "\"coordinates\":[[[max,min],[min,min],[min,max],[max,max],[max,min]]]\n",
    "\n",
    "For example, consider the plume from coal site with coordinates [36.75,\t109.76] (this is the 1st plume in [all TROPOMI detected plumes for 2021. (Schuit et al. 2023)](https://zenodo.org/records/8087134)). The coordinates of the rectangle are\n",
    "\n",
    "\"coordinates\":[[[110.01,36.5],[109.51,36.5],[109.51,37.0 ],[110.01,37.0],[110.01,36.5]]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8bff22f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# List of areas of interest used as an argument in aggregate_spatial. The coordinates correspond to the 0.5 x 0.5 deg rectangles \n",
    "# around the first two plumes from coal sites (plume [36.75, 109.76] and plume [37.53, 110.75]) in the dataset.\n",
    "features = {\"type\": \"FeatureCollection\", \"features\": [\n",
    "    {\n",
    "        \"type\": \"Feature\", \"properties\": {},\n",
    "        \"geometry\": {\"type\": \"Polygon\", \"coordinates\": [[\n",
    "            [110.01, 36.5],[109.51, 36.5],[109.51, 37.0],[110.01, 37.0],[110.01, 36.5]\n",
    "        ]]}\n",
    "    },\n",
    "    {\n",
    "        \"type\": \"Feature\", \"properties\": {},\n",
    "        \"geometry\": {\"type\": \"Polygon\", \"coordinates\": [[\n",
    "            [111.0, 37.28],[110.5, 37.28],[110.5, 37.78],[111.0, 37.78],[111.0, 37.28]\n",
    "        ]]}\n",
    "    }\n",
    "]}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d0b53921",
   "metadata": {},
   "source": [
    "**3. Add reducer and initiate the job**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f8b4f70",
   "metadata": {},
   "outputs": [],
   "source": [
    "s5_CH4_aggregation = s5_CH4.aggregate_spatial(\n",
    "    geometries=features,\n",
    "    reducer=\"mean\",\n",
    ")\n",
    "s5_CH4_aggregation = s5_CH4_aggregation.save_result(format=\"JSON\")\n",
    "job=s5_CH4_aggregation.create_job(title=\"s5_CH4_aggregation\")\n",
    "job.start()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f95bbe9f",
   "metadata": {},
   "outputs": [],
   "source": [
    "job"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c47b7d53",
   "metadata": {},
   "outputs": [],
   "source": [
    "results = job.get_results()\n",
    "results.download_file(\"s5_CH4_aggregation.json\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1789167a",
   "metadata": {},
   "outputs": [],
   "source": [
    "s3_aggregation = s3_all_bands.aggregate_spatial(\n",
    "    geometries=features,\n",
    "    reducer=\"mean\",\n",
    ")\n",
    "s3_aggregation = s3_aggregation.save_result(format=\"JSON\")\n",
    "job=s3_aggregation.create_job(title=\"s3_aggregation\")\n",
    "job.start()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0fcfedd4",
   "metadata": {},
   "outputs": [],
   "source": [
    "results = job.get_results()\n",
    "results.download_file(\"s3_aggregation.json\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14fbaee4",
   "metadata": {},
   "source": [
    "## From JSON to timeseries dataframe\n",
    "\n",
    "The openEO Python Client Library has a helper function that converts a JSON into pandas dataframe. From there, you can compute column-wise statistics (min, max, mean, sd and median)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd8f513f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import pandas as pd\n",
    "from openeo.rest.conversions import timeseries_json_to_pandas\n",
    "\n",
    "with open(\"s5_CH4_aggregation.json\") as f:\n",
    "    data = json.load(f)\n",
    "\n",
    "df_s5 = timeseries_json_to_pandas(data)\n",
    "df_s5.index = pd.to_datetime(df_s5.index)\n",
    "\n",
    "with open(\"s3_aggregation.json\") as f:\n",
    "    data = json.load(f)\n",
    "\n",
    "df_s3 = timeseries_json_to_pandas(data)\n",
    "df_s3.index = pd.to_datetime(df_s3.index)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5920fac5",
   "metadata": {},
   "source": [
    "## Divide and conquer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b908dc41",
   "metadata": {},
   "source": [
    "Note that the csv file contains the locations of 1913 plumes that come from oil, gas or coal locations. This is quite a bit of loading per user, and Copernicus has user quotas, which are pretty cryptic, so I cannot guarantee that a batch job of all computations for all plumes would not run out of quota. Instead, you are encouraged to divide and conquer: either with your friends and/or by creating several user profiles on Copernicus. Some tips:\n",
    "- you can re-connect to a job at another time, by another script/notebook or even with another openEO client within 7 days;\n",
    "- Copernicus servers can process up to 2 batch jobs per user simultaneously;\n",
    "- Copernicus has certain usage quotas. To get a sense of where you are at the moment, you can check you status using [Sentinel Hub Services Dashboard](https://shapps.dataspace.copernicus.eu/dashboard/#/). Also, running the 'job' command after a job is finished will tell you have many credits were used.\n",
    "\n",
    "**Minimal requirement for passing assignment 3:** the SVM needs to be performed on at least 50 plumes; one student should load the data for at least 20 plumes."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b9411811",
   "metadata": {},
   "source": [
    "## Increase batch resources\n",
    "\n",
    "In principle, you can also increase the computational resources available for your batch jobs, as shown in [these instructions](https://documentation.dataspace.copernicus.eu/APIs/openEO/job_config.html). Some warnings:\n",
    "- I do not know how far these can be pushed, but you could use the options for server settings when you try to access Copernicus JupyterLab [here](https://dataspace.copernicus.eu/analyse/jupyterlab)\n",
    "- My understanding is that a substantial portion of the total job execution time is the queue waiting time"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
